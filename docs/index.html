<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer from Scratch - A Comic Guide</title>
    <meta name="description"
        content="A generic visual comic book guide to the Transformer Deep Learning architecture. Learn about Attention Mechanisms, Embeddings, and Neural Networks through detailed comic panels and math explanations.">
    <meta name="keywords"
        content="Transformer, AI, Deep Learning, Comic, Machine Learning, Attention Mechanism, Neural Networks, Educational, Visualization">
    <meta property="og:title" content="Transformer from Scratch - A Comic Guide">
    <meta property="og:description" content="Read the visual guide to Transformers.">
    <meta property="og:type" content="website">
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Bangers&family=Comic+Neue:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap"
        rel="stylesheet">

    <!-- MathJax -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Highlight.js -->
    <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>

    <style>
        :root {
            --bg-color: #fce4ec;
            /* Light pinkish comic background */
            --text-color: #212121;
            --panel-bg: #ffffff;
            --border-color: #000000;
            --accent-color: #ffca28;
            /* Amber */
            --header-font: 'Bangers', cursive;
            --body-font: 'Comic Neue', cursive;
            --sidebar-width: 250px;
        }

        body.dark-mode {
            --bg-color: #1a1a1a;
            --text-color: #eaeaea;
            --panel-bg: #2d2d2d;
            --border-color: #555555;
            --accent-color: #ff8f00;
        }

        * {
            box-sizing: border-box;
        }

        body {
            margin: 0;
            font-family: var(--body-font);
            background-color: var(--bg-color);
            color: var(--text-color);
            font-size: 1.1rem;
            display: flex;
        }

        /* Sidebar Navigation */
        aside {
            width: var(--sidebar-width);
            height: 100vh;
            position: fixed;
            left: 0;
            top: 0;
            background-color: var(--panel-bg);
            border-right: 3px solid var(--border-color);
            overflow-y: auto;
            padding: 1rem;
            z-index: 100;
        }

        aside h1 {
            font-family: var(--header-font);
            font-size: 1.8rem;
            margin-bottom: 1rem;
            text-align: center;
        }

        aside nav ul {
            list-style: none;
            padding: 0;
        }

        aside nav ul li {
            margin-bottom: 0.5rem;
        }

        aside nav ul li a {
            text-decoration: none;
            color: var(--text-color);
            font-weight: bold;
            display: block;
            padding: 0.5rem;
            border: 2px solid transparent;
            border-radius: 8px;
            transition: all 0.2s;
        }

        aside nav ul li a:hover {
            background-color: var(--accent-color);
            border-color: var(--border-color);
            color: #000;
        }

        /* Main Content */
        main {
            margin-left: var(--sidebar-width);
            flex: 1;
            padding: 2rem;
            max-width: 1200px;
            /* Keep consistent width for readability */
            margin-right: auto;
        }

        .comic-page {
            background-color: var(--panel-bg);
            border: 4px solid var(--border-color);
            box-shadow: 10px 10px 0px rgba(0, 0, 0, 0.2);
            margin-bottom: 3rem;
            padding: 2rem;
            position: relative;
            break-inside: avoid;
            /* Prevent page break inside comic page for PDF */
        }

        .page-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            border-bottom: 3px dashed var(--border-color);
            padding-bottom: 1rem;
            margin-bottom: 1.5rem;
        }

        .page-header h2 {
            font-family: var(--header-font);
            font-size: 2.5rem;
            margin: 0;
        }

        .page-number {
            font-family: var(--header-font);
            font-size: 1.5rem;
            background-color: var(--accent-color);
            padding: 0.5rem 1rem;
            border: 2px solid var(--border-color);
            border-radius: 50%;
        }

        .comic-layout {
            display: flex;
            flex-direction: column;
            gap: 2rem;
        }

        @media (min-width: 768px) {
            .comic-layout {
                flex-direction: row;
                align-items: flex-start;
            }

            .panel-image {
                flex: 1;
                max-width: 50%;
            }

            .panel-content {
                flex: 1;
            }
        }

        .panel-image img {
            width: 100%;
            height: auto;
            border: 3px solid var(--border-color);
            border-radius: 4px;
            box-shadow: 5px 5px 0px rgba(0, 0, 0, 0.1);
        }

        .panel-content {
            font-size: 1.1rem;
            line-height: 1.6;
        }

        /* Narratives look like speech bubbles/captions */
        .narrative-box {
            background-color: #fff9c4;
            /* Light yellow */
            border: 2px solid var(--border-color);
            padding: 1rem;
            border-radius: 12px;
            margin-bottom: 1.5rem;
            position: relative;
            color: #000;
        }

        body.dark-mode .narrative-box {
            background-color: #ffd54f;
        }

        .technical-section h3 {
            font-family: var(--header-font);
            color: #d32f2f;
            /* Dark red */
            font-size: 1.8rem;
            border-bottom: 2px solid #d32f2f;
            display: inline-block;
        }

        body.dark-mode .technical-section h3 {
            color: #ff5252;
            border-bottom-color: #ff5252;
        }

        /* Dark Mode Toggle */
        .theme-toggle {
            position: fixed;
            top: 20px;
            right: 20px;
            z-index: 101;
        }

        .theme-toggle button {
            background-color: var(--accent-color);
            border: 3px solid var(--border-color);
            padding: 0.5rem 1rem;
            font-family: var(--header-font);
            font-size: 1.2rem;
            cursor: pointer;
            box-shadow: 2px 2px 0px rgba(0, 0, 0, 0.5);
        }

        .theme-toggle button:active {
            transform: translate(2px, 2px);
            box-shadow: none;
        }

        /* Print Styles */
        @media print {

            aside,
            .theme-toggle {
                display: none;
            }

            main {
                margin: 0;
                width: 100%;
                max-width: 100%;
            }

            .comic-page {
                box-shadow: none;
                border: 2px solid #000;
                page-break-after: always;
            }

            body {
                background-color: #fff;
            }
        }

        code {
            font-family: 'Consolas', 'Monaco', monospace;
            background-color: rgba(0, 0, 0, 0.1);
            padding: 2px 4px;
            border-radius: 4px;
        }

        pre code {
            display: block;
            padding: 1em;
            overflow-x: auto;
        }
    </style>
</head>

<body>

    <div class="theme-toggle">
        <button onclick="document.body.classList.toggle('dark-mode')">Toggle Dark Mode</button>
    </div>

    <aside>
        <h1>Transformer Comic</h1>
        <nav>
            <ul>
                
                <li>
                    <a href="#page-1">
                        Page 1:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">The Translation Challenge</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-2">
                        Page 2:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">The Transformer Arrives</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-3">
                        Page 3:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">Token Embedding - Words to Numbers</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-4">
                        Page 4:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">Position Matters!</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-5">
                        Page 5:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">Query, Key, Value - The Three Musketeers</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-6">
                        Page 6:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">The Attention Score Calculation</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-7">
                        Page 7:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">Softmax - Converting to Probabilities</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-8">
                        Page 8:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">Weighted Sum - Gathering Information</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-9">
                        Page 9:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">Eight Heads Are Better Than One</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-10">
                        Page 10:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">What Each Head Learns</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-11">
                        Page 11:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">The Power-Up Station</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-12">
                        Page 12:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">Why FFN After Attention?</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-13">
                        Page 13:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">The Skip Connection Highway</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-14">
                        Page 14:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">Layer Normalization - Keeping Things Stable</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-15">
                        Page 15:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">Assembling the Encoder</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-16">
                        Page 16:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">The Decoder's Special Power</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-17">
                        Page 17:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">Encoder-Decoder Attention Bridge</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-18">
                        Page 18:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">From Vectors to Words</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-19">
                        Page 19:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">Training - Teacher Forcing</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-20">
                        Page 20:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">The Transformer Revolution</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-21">
                        Page 21:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">Hyperparameters Cheatsheet</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-22">
                        Page 22:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">The Math Behind Attention</span>
                    </a>
                </li>
                
                <li>
                    <a href="#page-23">
                        Page 23:<br>
                        <span style="font-weight: normal; font-size: 0.9em;">Implementation Tricks & Optimizations</span>
                    </a>
                </li>
                
            </ul>
        </nav>
    </aside>

    <main>
        
        <article id="page-1" class="comic-page">
            <div class="page-header">
                <h2>The Translation Challenge</h2>
                <div class="page-number">#1</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> Meet Alex, a language enthusiast trying to translate "The cat sat on the mat" to French using old methods.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_01.png" alt="Comic book panel showing a frustrated character with thought bubbles containing fading words, arrows showing sequential processing left to right, information getting dimmer with each step, vibrant comic book art style">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Comic book panel showing a frustrated character with thought bubbles containing fading words, arrows showing sequential processing left to right, information getting dimmer with each step, vibrant comic book art style
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>The Old Way (RNNs): Sequential processing - one word at a time</h3>
<p><strong>The Problem:</strong> "Cat" processed &rarr; "sat" processed &rarr; "on" processed... By the time we reach "mat", the network has forgotten important context about "cat"</p>
<p><strong>Memory Loss Visualization:</strong> Fading colors showing information degradation</p>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 1
            </div>
        </article>
        
        <article id="page-2" class="comic-page">
            <div class="page-header">
                <h2>The Transformer Arrives</h2>
                <div class="page-number">#2</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> A superhero-like character (The Transformer) appears, declaring "I can see ALL words at ONCE!"
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_02.png" alt="Dynamic comic book splash page with a superhero character surrounded by floating words all connected by glowing energy lines, words illuminated simultaneously, action-packed composition, bold colors">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Dynamic comic book splash page with a superhero character surrounded by floating words all connected by glowing energy lines, words illuminated simultaneously, action-packed composition, bold colors
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Parallel Processing: All tokens processed simultaneously</h3>
<p><strong>Full Context Awareness:</strong> Every word can "attend" to every other word</p>
<p><strong>The Architecture Overview:</strong> High-level block diagram</p>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 1
            </div>
        </article>
        
        <article id="page-3" class="comic-page">
            <div class="page-header">
                <h2>Token Embedding - Words to Numbers</h2>
                <div class="page-number">#3</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> Words entering a magical portal and emerging as glowing number vectors.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_03.png" alt="Comic panel showing words passing through a glowing portal, emerging as colorful floating number arrays, matrix visualization in background, mystical energy effects, vibrant technical diagram aesthetic">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Comic panel showing words passing through a glowing portal, emerging as colorful floating number arrays, matrix visualization in background, mystical energy effects, vibrant technical diagram aesthetic
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Input: "The cat sat"</h3>
<p><strong>Tokenization:</strong> ["The", "cat", "sat"]</p>
<p><strong>Token IDs:</strong> [245, 3421, 2156]</p>
<p><strong>Embedding Dimension:</strong> \( d_{model} = 512 \)</p>
<p>Each token &rarr; 512-dimensional vector<br>
"cat" &rarr; [0.234, -0.567, 0.123, ..., 0.891] (512 values)</p>
<h4>Mathematical Detail:</h4>
<ul>
<li><strong>Embedding Matrix:</strong> \( V \times d_{model} \) (V = vocab size, typically 50,000)</li>
<li><strong>Lookup operation:</strong> One-hot encoding &times; Embedding Matrix</li>
<li><strong>Why 512?:</strong> Balance between expressiveness and computation</li>
</ul>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 2
            </div>
        </article>
        
        <article id="page-4" class="comic-page">
            <div class="page-header">
                <h2>Position Matters!</h2>
                <div class="page-number">#4</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> Two identical words in different positions need different encodings.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_04.png" alt="Split panel comic showing identical twin word-characters, one labeled 'Position 1' and one 'Position 3', each glowing with different colored wave patterns overlaid, sine and cosine waves visualized as auras, technical elegance">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Split panel comic showing identical twin word-characters, one labeled 'Position 1' and one 'Position 3', each glowing with different colored wave patterns overlaid, sine and cosine waves visualized as auras, technical elegance
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Positional Encoding Formula:</h3>
<p>$$ PE(pos, 2i) = \sin(pos / 10000^{2i/d_{model}}) $$</p>
<p>$$ PE(pos, 2i+1) = \cos(pos / 10000^{2i/d_{model}}) $$</p>

<h4>Example for position 0, 1, 2:</h4>
<ul>
<li><strong>Position 0:</strong> [0.000, 1.000, 0.000, 1.000, ...]</li>
<li><strong>Position 1:</strong> [0.841, 0.540, 0.099, 0.995, ...]</li>
<li><strong>Position 2:</strong> [0.909, -0.416, 0.198, 0.980, ...]</li>
</ul>
<p><strong>Final Input</strong> = Token Embedding + Positional Encoding</p>
<h4>Why Sinusoidal?</h4>
<ul>
<li>Unique pattern for each position</li>
<li>Can extrapolate to unseen sequence lengths</li>
<li>Geometric relationship between positions</li>
</ul>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 2
            </div>
        </article>
        
        <article id="page-5" class="comic-page">
            <div class="page-header">
                <h2>Query, Key, Value - The Three Musketeers</h2>
                <div class="page-number">#5</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> Three transformations of each word, working together to find relationships.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_05.png" alt="Three superhero characters emerging from a single word, labeled Q, K, V, each with distinct color scheme (blue, red, green), showing transformation arrows from original word, dynamic action poses, comic book superhero style">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Three superhero characters emerging from a single word, labeled Q, K, V, each with distinct color scheme (blue, red, green), showing transformation arrows from original word, dynamic action poses, comic book superhero style
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<p>For each token embedding \( x \) (512-dim):</p>
<h4>Query (Q): "What am I looking for?"</h4>
<p>$$ Q = x \times W_Q \quad (512 \times 64 \rightarrow 64\text{-dim}) $$</p>

<h4>Key (K): "What do I contain?"</h4>
<p>$$ K = x \times W_K \quad (512 \times 64 \rightarrow 64\text{-dim}) $$</p>

<h4>Value (V): "What information do I carry?"</h4>
<p>$$ V = x \times W_V \quad (512 \times 64 \rightarrow 64\text{-dim}) $$</p>

<p><strong>Why 64?</strong> \( d_k = d_{model} / num\_heads = 512 / 8 = 64 \)</p>
<h4>Intuition:</h4>
<ul>
<li><strong>Query:</strong> "I'm the word 'sat', I need to know WHO sat"</li>
<li><strong>Key:</strong> "I'm the word 'cat', I describe a WHO"</li>
<li><strong>Value:</strong> "I'm 'cat', here's my semantic meaning to pass along"</li>
</ul>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 3
            </div>
        </article>
        
        <article id="page-6" class="comic-page">
            <div class="page-header">
                <h2>The Attention Score Calculation</h2>
                <div class="page-number">#6</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> Q and K shake hands to determine compatibility.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_06.png" alt="Comic panel showing word-characters shaking hands, energy bolts between them with floating numbers, scoreboard in background showing compatibility scores, action lines indicating strength of connection, vibrant energy effects">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Comic panel showing word-characters shaking hands, energy bolts between them with floating numbers, scoreboard in background showing compatibility scores, action lines indicating strength of connection, vibrant energy effects
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Step 1: Compute Compatibility</h3>
<p>$$ \text{Attention Score} = \frac{Q \cdot K^T}{\sqrt{d_k}} $$</p>
<h4>Example for "sat" attending to all words:</h4>
<ul>
<li>\( Q_{sat} \cdot K_{The} = 2.3 \)</li>
<li>\( Q_{sat} \cdot K_{cat} = 8.7 \) &larr; <strong>High score! Related!</strong></li>
<li>\( Q_{sat} \cdot K_{sat} = 3.1 \)</li>
</ul>
<h3>Step 2: Scale by \( \sqrt{d_k} = \sqrt{64} = 8 \)</h3>
<p><strong>Scaled Scores:</strong> [0.29, 1.09, 0.39]</p>
<h4>Why divide by \( \sqrt{d_k} \)?</h4>
<ul>
<li>Prevents scores from getting too large</li>
<li>Keeps gradients stable</li>
<li>Critical for training!</li>
</ul>
<pre><code class="language-python"># Matrix Form (for sequence length n=3):
#       [The] [cat] [sat]
# [The] 0.29  0.45  0.31
# [cat] 0.35  0.88  0.41
# [sat] 0.29  1.09  0.39
</code></pre>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 3
            </div>
        </article>
        
        <article id="page-7" class="comic-page">
            <div class="page-header">
                <h2>Softmax - Converting to Probabilities</h2>
                <div class="page-number">#7</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> Raw scores transformed into a probability distribution.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_07.png" alt="Comic panel showing a pie chart emerging from number scores, word-characters sized proportionally to their attention weights, largest character glowing brightest, probability percentages floating above, clean technical illustration style">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Comic panel showing a pie chart emerging from number scores, word-characters sized proportionally to their attention weights, largest character glowing brightest, probability percentages floating above, clean technical illustration style
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Softmax Function:</h3>
<p>$$ \text{attention\_weights} = \text{softmax}(\text{scores}) = \frac{\exp(\text{scores})}{\sum \exp(\text{scores})} $$</p>

<h4>For "sat" row: [0.29, 1.09, 0.39]</h4>
<p>\( \exp([0.29, 1.09, 0.39]) = [1.34, 2.97, 1.48] \)</p>
<p>Sum = 5.79</p>

<p><strong>Softmax:</strong> [1.34/5.79, 2.97/5.79, 1.48/5.79] = [0.23, 0.51, 0.26]</p>

<h4>Interpretation:</h4>
<ul>
<li>23% attention to "The"</li>
<li>51% attention to "cat" &larr; <strong>Most relevant!</strong></li>
<li>26% attention to "sat" itself</li>
</ul>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 3
            </div>
        </article>
        
        <article id="page-8" class="comic-page">
            <div class="page-header">
                <h2>Weighted Sum - Gathering Information</h2>
                <div class="page-number">#8</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> Values combined according to attention weights to create enriched representation.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_08.png" alt="Comic panel showing three translucent ghost-like value characters merging into one solid character, weighted by size/opacity, mathematical formula glowing in background, fusion energy effects, dramatic transformation scene">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Comic panel showing three translucent ghost-like value characters merging into one solid character, weighted by size/opacity, mathematical formula glowing in background, fusion energy effects, dramatic transformation scene
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Output = Attention_weights &times; Values</h3>
<p>For "sat":</p>
<p>$$ output_{sat} = 0.23 \times V_{The} + 0.51 \times V_{cat} + 0.26 \times V_{sat} $$</p>

<p>If \( V_{The} = [0.1, 0.2, ..., 0.3] \) (64-dim)<br>
\( V_{cat} = [0.8, 0.9, ..., 0.7] \) (64-dim)<br>
\( V_{sat} = [0.3, 0.4, ..., 0.2] \) (64-dim)</p>

<p>\( output_{sat} = [0.51, 0.58, ..., 0.49] \) (64-dim)</p>

<p><strong>Now "sat" has learned it's related to "cat"!</strong></p>
<h4>Complete Attention Formula:</h4>
<p>$$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V $$</p>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 3
            </div>
        </article>
        
        <article id="page-9" class="comic-page">
            <div class="page-header">
                <h2>Eight Heads Are Better Than One</h2>
                <div class="page-number">#9</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> The transformer splits into 8 parallel versions, each focusing on different aspects.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_09.png" alt="Comic splash page with 8 identical characters in circle formation, each with different colored aura, viewing same scene from different angles, spider-web of connections between all characters, kaleidoscopic effect, multi-perspective composition">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Comic splash page with 8 identical characters in circle formation, each with different colored aura, viewing same scene from different angles, spider-web of connections between all characters, kaleidoscopic effect, multi-perspective composition
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Why 8 heads?</h3>
<ul>
<li>Head 1 might learn: syntactic relationships (subject-verb)</li>
<li>Head 2 might learn: semantic similarity</li>
<li>Head 3 might learn: positional proximity</li>
<li>Head 4 might learn: co-reference (pronouns)</li>
<li>... and so on</li>
</ul>

<p>Each head: \( d_k = d_v = 512/8 = 64 \) dimensions</p>

<h4>Parallel Processing:</h4>
<pre><code class="language-python">head_1 = Attention(Q_1, K_1, V_1)  # (n x 64)
head_2 = Attention(Q_2, K_2, V_2)  # (n x 64)
# ...
head_8 = Attention(Q_8, K_8, V_8)  # (n x 64)
</code></pre>

<p><strong>Concatenate:</strong> [head_1 | head_2 | ... | head_8] &rarr; (n &times; 512)</p>
<p><strong>Final Projection:</strong> Concat &times; \( W_O \) &rarr; (n &times; 512)</p>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 4
            </div>
        </article>
        
        <article id="page-10" class="comic-page">
            <div class="page-header">
                <h2>What Each Head Learns</h2>
                <div class="page-number">#10</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> Visualization of different attention patterns discovered by different heads.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_10.png" alt="Comic panel grid showing 4 mini-scenes, each depicting different types of word connections with different colored energy beams, syntactic connections shown as rigid lines, semantic as flowing curves, local as tight spirals, diverse visual metaphors">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Comic panel grid showing 4 mini-scenes, each depicting different types of word connections with different colored energy beams, syntactic connections shown as rigid lines, semantic as flowing curves, local as tight spirals, diverse visual metaphors
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Example Sentence: "The cat sat on the mat because it was tired"</h3>

<h4>Head 1 - Syntactic (Subject-Verb):</h4>
<p>"cat" &rarr; "sat" (strong)</p>
<p>"it" &rarr; "was" (strong)</p>

<h4>Head 2 - Long-range Dependencies:</h4>
<p>"it" &rarr; "cat" (pronoun resolution)</p>

<h4>Head 3 - Local Context:</h4>
<p>"sat" &rarr; "on" (adjacent words)</p>
<p>"on" &rarr; "the" (adjacent words)</p>

<h4>Head 4 - Semantic:</h4>
<p>"cat" &rarr; "tired" (living things get tired)</p>
<p>Attention Pattern Matrices for different heads shown side-by-side</p>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 4
            </div>
        </article>
        
        <article id="page-11" class="comic-page">
            <div class="page-header">
                <h2>The Power-Up Station</h2>
                <div class="page-number">#11</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> Each token visits a power-up station independently (position-wise).
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_11.png" alt="Comic panel showing a word-character entering a power-up chamber, expanding into larger glowing form (2048), then compressing back down to original size but now glowing brighter, transformation sequence with energy effects, video game power-up aesthetic">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Comic panel showing a word-character entering a power-up chamber, expanding into larger glowing form (2048), then compressing back down to original size but now glowing brighter, transformation sequence with energy effects, video game power-up aesthetic
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Feed-Forward Network (FFN):</h3>
<p>$$ \text{FFN}(x) = \max(0, x W_1 + b_1) W_2 + b_2 $$</p>

<h4>Layer dimensions:</h4>
<ul>
<li>\( W_1 \): 512 &times; 2048 (expansion)</li>
<li>\( W_2 \): 2048 &times; 512 (projection back)</li>
</ul>

<h4>Why 2048?</h4>
<ul>
<li>4&times; expansion factor is standard</li>
<li>Gives network more capacity</li>
<li>Creates non-linear transformations</li>
</ul>

<h4>ReLU Activation:</h4>
<p>\( \max(0, x) \) - kills negative values. Introduces non-linearity - crucial for learning complex patterns!</p>

<p><strong>Applied to EACH token independently (unlike attention)</strong></p>
<p>Step-by-step for one token:</p>
<pre>
Input: [512-dim vector]
  &darr; x W_1 + b_1
Intermediate: [2048-dim vector]
  &darr; ReLU
Activated: [2048-dim vector] (some values zeroed)
  &darr; x W_2 + b_2
Output: [512-dim vector]
</pre>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 5
            </div>
        </article>
        
        <article id="page-12" class="comic-page">
            <div class="page-header">
                <h2>Why FFN After Attention?</h2>
                <div class="page-number">#12</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> Attention gathers information, FFN processes it individually.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_12.png" alt="Two-panel comic: Panel 1 shows characters in circle sharing information with crossed energy beams (attention), Panel 2 shows same characters each in individual meditation pose with internal glow (FFN), contrasting collaborative vs individual processing">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Two-panel comic: Panel 1 shows characters in circle sharing information with crossed energy beams (attention), Panel 2 shows same characters each in individual meditation pose with internal glow (FFN), contrasting collaborative vs individual processing
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Division of Labor:</h3>

<h4>Multi-Head Attention:</h4>
<ul>
<li>Mixes information ACROSS tokens</li>
<li>"cat" learns from "sat", "the", etc.</li>
<li>Global information flow</li>
</ul>

<h4>Feed-Forward Network:</h4>
<ul>
<li>Processes EACH token independently</li>
<li>Refines the combined information</li>
<li>Local transformation</li>
</ul>

<h3>Together:</h3>
<ol>
<li>Attention: "Gather context from other words"</li>
<li>FFN: "Now think deeply about what I've learned"</li>
</ol>
<p><strong>Analogy:</strong> Attention = Group discussion, FFN = Individual reflection</p>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 5
            </div>
        </article>
        
        <article id="page-13" class="comic-page">
            <div class="page-header">
                <h2>The Skip Connection Highway</h2>
                <div class="page-number">#13</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> Information has a shortcut path to preserve original signal.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_13.png" alt="Comic panel showing two paths - main winding road with obstacles (sublayer processing) and parallel highway (skip connection), both merging at end, arrows showing information flow, road/highway metaphor, clear path visualization">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Comic panel showing two paths - main winding road with obstacles (sublayer processing) and parallel highway (skip connection), both merging at end, arrows showing information flow, road/highway metaphor, clear path visualization
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Problem: Deep networks lose information through layers</h3>

<h4>Residual Connection:</h4>
<p>$$ \text{output} = \text{LayerNorm}(x + \text{Sublayer}(x)) $$</p>

<p>Where Sublayer is either:</p>
<ul>
<li>Multi-Head Attention</li>
<li>Feed-Forward Network</li>
</ul>

<h4>Example:</h4>
<p>\( x_{input} = [0.5, 0.3, 0.8, ...] \) (512-dim)</p>
<p>\( \text{attention\_output} = [0.1, 0.2, 0.3, ...] \) (512-dim)</p>

<p><strong>With residual:</strong><br>
\( x_{next} = x_{input} + \text{attention\_output} = [0.6, 0.5, 1.1, ...] \)</p>

<h4>Why crucial?</h4>
<ul>
<li>Helps gradients flow during backprop</li>
<li>Prevents vanishing gradients</li>
<li>Allows training very deep networks (6, 12, 24+ layers)</li>
</ul>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 6
            </div>
        </article>
        
        <article id="page-14" class="comic-page">
            <div class="page-header">
                <h2>Layer Normalization - Keeping Things Stable</h2>
                <div class="page-number">#14</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> A balancing act to keep values in reasonable range.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_14.png" alt="Comic panel showing scattered numbers of wildly different sizes being normalized into similar-sized balanced figures on a scale, before/after comparison, balance beam metaphor with numbers as weights, equilibrium visualization">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Comic panel showing scattered numbers of wildly different sizes being normalized into similar-sized balanced figures on a scale, before/after comparison, balance beam metaphor with numbers as weights, equilibrium visualization
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Layer Normalization:</h3>
<p>$$ \text{LN}(x) = \gamma \times \frac{x - \mu}{\sigma} + \beta $$</p>

<p>For each token vector (512 values):<br>
\( \mu \) = mean of the 512 values<br>
\( \sigma \) = standard deviation of the 512 values<br>
\( \gamma, \beta \) = learned parameters (scale and shift)</p>

<h4>Example:</h4>
<p>\( x = [100, 200, 150, 50] \) (simplified to 4-dim)</p>
<p>\( \mu = (100 + 200 + 150 + 50) / 4 = 125 \)</p>
<p>\( \sigma = \sqrt{\text{variance}} \approx 55.9 \)</p>

<p><strong>Normalized:</strong> [(100-125)/55.9, (200-125)/55.9, (150-125)/55.9, (50-125)/55.9] = [-0.45, 1.34, 0.45, -1.34]</p>

<p>Then scale and shift with learned \( \gamma, \beta \)</p>
<h4>Why?</h4>
<ul>
<li>Prevents exploding/vanishing activations</li>
<li>Stabilizes training</li>
<li>Allows higher learning rates</li>
</ul>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 6
            </div>
        </article>
        
        <article id="page-15" class="comic-page">
            <div class="page-header">
                <h2>Assembling the Encoder</h2>
                <div class="page-number">#15</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> All components come together into one mighty encoder block.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_15.png" alt="Detailed technical schematic comic page showing complete encoder block as a machine/factory, input tokens entering at bottom, passing through labeled stages (attention chamber, residual highways, FFN station, normalization gates), isometric technical drawing style, cutaway view showing internal processes">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Detailed technical schematic comic page showing complete encoder block as a machine/factory, input tokens entering at bottom, passing through labeled stages (attention chamber, residual highways, FFN station, normalization gates), isometric technical drawing style, cutaway view showing internal processes
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Single Encoder Block:</h3>

<pre>
Input: x  (n x 512)
  &darr;
1. Multi-Head Attention
   attention_out = MultiHeadAttention(x, x, x)
  &darr;
2. Add & Norm
   x = LayerNorm(x + attention_out)
  &darr;
3. Feed-Forward Network
   ffn_out = FFN(x)
  &darr;
4. Add & Norm
   x = LayerNorm(x + ffn_out)
  &darr;
Output: x  (n x 512)
</pre>

<p><strong>Standard Transformer:</strong> Stack 6 identical encoder blocks</p>
<p><strong>GPT-3:</strong> 96 layers!</p>
<h4>Complete Architecture Diagram:</h4>
<p>[Input Embedding + Positional Encoding] &rarr; [Encoder 1] &rarr; [Encoder 2] &rarr; ... &rarr; [Encoder 6] &rarr; [Output]</p>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 7
            </div>
        </article>
        
        <article id="page-16" class="comic-page">
            <div class="page-header">
                <h2>The Decoder's Special Power</h2>
                <div class="page-number">#16</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> Decoder can't peek into the future (masked attention).
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_16.png" alt="Comic panel showing decoder-character with one-way glasses/visor looking only backward at previous tokens (shown as fading trail), future tokens shown as blurred/locked, mask visualization as a diagonal barrier, time-travel prevention metaphor">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Comic panel showing decoder-character with one-way glasses/visor looking only backward at previous tokens (shown as fading trail), future tokens shown as blurred/locked, mask visualization as a diagonal barrier, time-travel prevention metaphor
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Decoder Differences:</h3>

<h4>1. Masked Self-Attention</h4>
<ul>
<li>Can only attend to previous positions</li>
<li>Prevents "cheating" during training</li>
</ul>

<pre>
Mask Matrix (for 4 tokens):
       t0  t1  t2  t3
t0  [  0  -&infin;  -&infin;  -&infin; ]
t1  [  0   0  -&infin;  -&infin; ]
t2  [  0   0   0  -&infin; ]
t3  [  0   0   0   0 ]
</pre>
<p>After softmax, -&infin; becomes 0 (no attention)</p>

<h4>2. Encoder-Decoder Attention</h4>
<ul>
<li>Query from decoder</li>
<li>Key & Value from encoder</li>
<li>"What did the input say?"</li>
</ul>

<h4>3. Feed-Forward (same as encoder)</h4>
<p><strong>Autoregressive Generation:</strong> Input: "Hello" &rarr; Output: "world"</p>
<ol>
<li>Step 1: Generate "w" given "Hello"</li>
<li>Step 2: Generate "o" given "Hello w"</li>
<li>Step 3: Generate "r" given "Hello wo"</li>
<li>... and so on</li>
</ol>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 8
            </div>
        </article>
        
        <article id="page-17" class="comic-page">
            <div class="page-header">
                <h2>Encoder-Decoder Attention Bridge</h2>
                <div class="page-number">#17</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> Decoder asks questions to the encoder's knowledge.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_17.png" alt="Comic panel showing decoder character (left) sending question beam to encoder character (right), encoder holding knowledge book, information flowing back as answer beam, bridge of light between two towers metaphor, clear communication visualization">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Comic panel showing decoder character (left) sending question beam to encoder character (right), encoder holding knowledge book, information flowing back as answer beam, bridge of light between two towers metaphor, clear communication visualization
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Cross-Attention Mechanism:</h3>

<p><strong>Decoder:</strong> "I'm generating 'le' in French"<br>
Query: \( Q = \text{decoder\_state} \times W_Q \)</p>

<p><strong>Encoder:</strong> "I have info about 'the cat'"<br>
Key: \( K = \text{encoder\_output} \times W_K \)<br>
Value: \( V = \text{encoder\_output} \times W_V \)</p>

<h4>Attention Computation:</h4>
<p>$$ \text{attention} = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V $$</p>

<h4>Example:</h4>
<ul>
<li>French "le" strongly attends to English "the"</li>
<li>French "chat" strongly attends to English "cat"</li>
</ul>
<p><strong>This creates alignment between input and output!</strong></p>
<pre>
Attention Pattern (Translation example):
English:  The  cat  sat  on   mat
            &darr;   &darr;    &darr;   &darr;    &darr;
French:   Le  chat  ...  ...  ...
          &uarr;   &uarr;
       Strong connections where semantically aligned
</pre>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 8
            </div>
        </article>
        
        <article id="page-18" class="comic-page">
            <div class="page-header">
                <h2>From Vectors to Words</h2>
                <div class="page-number">#18</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> Final transformation back to vocabulary.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_18.png" alt="Comic panel showing final vector character entering a 'word printer' machine, probabilities shown as different sized word bubbles emerging, largest bubble highlighted as winner, lottery ball machine aesthetic with words instead of numbers">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Comic panel showing final vector character entering a 'word printer' machine, probabilities shown as different sized word bubbles emerging, largest bubble highlighted as winner, lottery ball machine aesthetic with words instead of numbers
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Final Linear Layer + Softmax:</h3>

<pre>
Decoder output: (n x 512)
  &darr; x W_output (512 x vocab_size)
Logits: (n x 50,000)
  &darr; softmax
Probabilities: (n x 50,000)
</pre>

<h4>Example for next token prediction:</h4>
<pre>
[
  "the": 0.001,
  "cat": 0.542,  &larr; Highest! Predict "cat"
  "dog": 0.123,
  "sat": 0.089,
  ...
  (50,000 words total)
]
</pre>

<h4>Sampling Strategies:</h4>
<ol>
<li><strong>Greedy:</strong> Always pick highest probability</li>
<li><strong>Top-k:</strong> Sample from top k tokens</li>
<li><strong>Nucleus (top-p):</strong> Sample from smallest set summing to p</li>
<li><strong>Temperature:</strong> Control randomness</li>
</ol>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 9
            </div>
        </article>
        
        <article id="page-19" class="comic-page">
            <div class="page-header">
                <h2>Training - Teacher Forcing</h2>
                <div class="page-number">#19</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> How the transformer learns from examples.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_19.png" alt="Comic panel showing teacher character pointing to target answer on blackboard, student transformer comparing its answer, error shown as red distance/gap between predicted and actual, adjustment arrows showing weight updates, classroom learning metaphor">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Comic panel showing teacher character pointing to target answer on blackboard, student transformer comparing its answer, error shown as red distance/gap between predicted and actual, adjustment arrows showing weight updates, classroom learning metaphor
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Training Process:</h3>

<p><strong>Input:</strong> "The cat sat on the mat"<br>
<strong>Target:</strong> "Le chat tait assis sur le tapis"</p>

<h4>Teacher Forcing:</h4>
<ul>
<li>Feed entire target sequence at once</li>
<li>Use masking to prevent future peeking</li>
<li>Compare predictions to actual next tokens</li>
</ul>

<h4>Loss Calculation (Cross-Entropy):</h4>
<p>At position i, predict token i+1:<br>
predicted_probs = [0.001, 0.542, ...]<br>
actual_token = "chat" (ID: 3421)</p>
<p>$$ \text{Loss} = -\log(\text{predicted\_probs}[3421]) = -\log(0.542) = 0.612 $$</p>
<p><strong>Total Loss = Average over all positions</strong></p>

<h4>Gradient Descent:</h4>
<ul>
<li>Compute gradients \( \partial \text{Loss}/\partial W \) for all weights</li>
<li>Update: \( W_{new} = W_{old} - \text{learning\_rate} \times \text{gradient} \)</li>
<li>Repeat for millions of examples!</li>
</ul>

<h4>Optimization (Adam):</h4>
<ul>
<li>Adaptive learning rates</li>
<li>Momentum for stable updates</li>
<li>Learning rate scheduling (warmup + decay)</li>
</ul>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 9
            </div>
        </article>
        
        <article id="page-20" class="comic-page">
            <div class="page-header">
                <h2>The Transformer Revolution</h2>
                <div class="page-number">#20</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> Summary of why transformers changed everything.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_20.png" alt="Epic final splash page showing transformer as superhero standing triumphant, background showing multiple application bubbles (text, images, molecules), before/after comparison of AI capabilities, celebratory heroic composition, inspirational technological advancement theme">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Epic final splash page showing transformer as superhero standing triumphant, background showing multiple application bubbles (text, images, molecules), before/after comparison of AI capabilities, celebratory heroic composition, inspirational technological advancement theme
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Revolutionary Aspects:</h3>

<ol>
<li><strong>Parallelization:</strong> RNN: O(n) sequential steps vs Transformer: O(1) parallel steps &rarr; 100&times; faster training!</li>
<li><strong>Long-Range Dependencies:</strong> Direct connections &rarr; Better at long texts!</li>
<li><strong>Interpretability:</strong> Attention weights = explicit relationships &rarr; Can visualize what model learned!</li>
<li><strong>Scalability:</strong> Architecture scales to billions of parameters (GPT-3: 175B) &rarr; Emergent capabilities!</li>
<li><strong>Transfer Learning:</strong> Pre-train once, fine-tune for many tasks &rarr; Democratized NLP!</li>
</ol>

<h4>Applications:</h4>
<ul>
<li><strong>BERT:</strong> Understanding (search, Q&A)</li>
<li><strong>GPT:</strong> Generation (writing, coding)</li>
<li><strong>T5:</strong> Translation, summarization</li>
<li><strong>Vision Transformers:</strong> Image classification</li>
<li><strong>AlphaFold:</strong> Protein structure prediction</li>
</ul>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 10
            </div>
        </article>
        
        <article id="page-21" class="comic-page">
            <div class="page-header">
                <h2>Hyperparameters Cheatsheet</h2>
                <div class="page-number">#21</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> Quick reference for transformer configurations.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_21.png" alt="Technical reference sheet styled as comic page with organized stat boxes, numbers highlighted in different colors, small character icons representing different model sizes, clean infographic style, easy-to-scan layout">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Technical reference sheet styled as comic page with organized stat boxes, numbers highlighted in different colors, small character icons representing different model sizes, clean infographic style, easy-to-scan layout
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Standard Transformer (Vaswani et al., 2017):</h3>
<pre>
d_model = 512          # Embedding dimension
d_ff = 2048            # FFN hidden size
num_heads = 8          # Attention heads
num_layers = 6         # Encoder/Decoder layers
d_k = d_v = 64         # Per-head dimensions
dropout = 0.1          # Dropout rate
vocab_size = 37000     # BPE tokens
max_seq_len = 512      # Max sequence length
</pre>

<h3>GPT-2 (Small):</h3>
<pre>
d_model = 768
d_ff = 3072
num_heads = 12
num_layers = 12
vocab_size = 50257
Parameters: 117M
</pre>

<h3>BERT (Base):</h3>
<pre>
d_model = 768
d_ff = 3072
num_heads = 12
num_layers = 12
vocab_size = 30522
Parameters: 110M
</pre>

<h3>Computational Cost:</h3>
<p>Self-Attention: \( O(n^2 \times d) \)<br>
FFN: \( O(n \times d^2) \)<br>
where n = sequence length, d = d_model</p>
<p>Memory for Attention: \( O(n^2 \times \text{num\_layers}) \)</p>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter 10
            </div>
        </article>
        
        <article id="page-22" class="comic-page">
            <div class="page-header">
                <h2>The Math Behind Attention</h2>
                <div class="page-number">#22</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> Complete Derivations.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_22.png" alt="Technical diagram page with mathematical equations, Greek symbols, matrix visualizations, proof steps with arrows, academic paper aesthetic with comic book color scheme, clean mathematical typography">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Technical diagram page with mathematical equations, Greek symbols, matrix visualizations, proof steps with arrows, academic paper aesthetic with comic book color scheme, clean mathematical typography
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>Scaled Dot-Product Attention:</h3>
<p>$$ \text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$</p>

<h4>Why \( \sqrt{d_k} \) scaling?</h4>
<p>Dot product \( Q \cdot K \) has variance proportional to \( d_k \). Without scaling, softmax saturates.</p>

<p><strong>Proof:</strong><br>
If \( Q, K \sim N(0,1) \), then \( Q \cdot K \sim N(0, d_k) \)<br>
Dividing by \( \sqrt{d_k} \): \( Q \cdot K/\sqrt{d_k} \sim N(0,1) \)<br>
Keeps variance constant regardless of dimension!</p>

<h4>Multi-Head Concatenation:</h4>
<p>$$ \text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O $$</p>
<p>where \( \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \)</p>

<h4>Parameter Count Analysis:</h4>
<ul>
<li>Per head: \( 3 \times (d_{model} \times d_k) = 3 \times 512 \times 64 = 98,304 \)</li>
<li>Total QKV: \( 8 \text{ heads} \times 98,304 = 786,432 \)</li>
<li>Output projection \( W^O \): \( 512 \times 512 = 262,144 \)</li>
<li>Total attention params: ~1M per layer</li>
</ul>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter Appendix
            </div>
        </article>
        
        <article id="page-23" class="comic-page">
            <div class="page-header">
                <h2>Implementation Tricks & Optimizations</h2>
                <div class="page-number">#23</div>
            </div>

            <div class="narrative-box">
                <strong>Narrator:</strong> Technical Content.
            </div>

            <div class="comic-layout">
                <div class="panel-image">
                    
                    <img src="images/page_23.png" alt="Comic page with multiple small technical diagrams, code snippets in speech bubbles, optimization shown as 'before/after' speed comparisons, engineering blueprint aesthetic, practical implementation focus">
                    
                    <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-top: 0.5rem;">
                        Prompt: Comic page with multiple small technical diagrams, code snippets in speech bubbles, optimization shown as 'before/after' speed comparisons, engineering blueprint aesthetic, practical implementation focus
                    </p>
                </div>

                <div class="panel-content technical-section">
                    
<h3>1. Masking Implementation:</h3>
<p>attention_scores + mask_matrix<br>
where mask = -1e9 (effectively -&infin;)</p>

<h3>2. Efficient Matrix Multiplication:</h3>
<p>Batch all heads: (batch &times; heads &times; seq &times; d_k)<br>
Single matrix multiply instead of loops</p>

<h3>3. Flash Attention (2022):</h3>
<p>Tiling technique to avoid materializing full attention matrix<br>
Memory: O(n) instead of O(n)<br>
Speed: 2-4&times; faster</p>

<h3>4. Position Encoding Alternatives:</h3>
<ul>
<li>Learned positional embeddings</li>
<li>Rotary Position Embedding (RoPE)</li>
<li>ALiBi (Attention with Linear Biases)</li>
</ul>

<h3>5. Activation Functions:</h3>
<ul>
<li>Original: ReLU</li>
<li>Modern: GELU, SwiGLU</li>
<li>GELU: \( x \cdot \Phi(x) \) where \( \Phi \) is Gaussian CDF</li>
<li>Smoother gradients, better performance</li>
</ul>

<h3>6. Attention Variants:</h3>
<ul>
<li>Sparse Attention: Only attend to subset</li>
<li>Local Attention: Fixed window</li>
<li>Longformer: Sliding window + global tokens</li>
<li>BigBird: Random + window + global</li>
</ul>

                </div>
            </div>

            <div style="position: absolute; bottom: 10px; right: 10px; font-size: 0.8rem; color: #999;">
                Chapter Appendix
            </div>
        </article>
        
    </main>

</body>

</html>